{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SRR13586106': 'NA12878',\n",
       " 'SRR13586016': 'NA12878',\n",
       " 'SRR13586007': 'NA12878',\n",
       " 'SRR13586011': 'NA24695',\n",
       " 'SRR13586014': 'NA24694',\n",
       " 'SRR13586012': 'NA24694',\n",
       " 'SRR13586015': 'NA24694',\n",
       " 'SRR13586013': 'NA24694',\n",
       " 'SRR13586020': 'NA24631',\n",
       " 'SRR13586019': 'NA24631',\n",
       " 'SRR13586018': 'NA24631',\n",
       " 'SRR13586026': 'NA24149'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''make a vcf file link for all datasets'''\n",
    "\n",
    "SEs = ['SRR13586011','SRR13586012','SRR13586013', \\\n",
    "            'SRR13586014', 'SRR13586015', 'SRR13586016', 'SRR13586018', 'SRR13586019', 'SRR13586020']\n",
    "\n",
    "PEs = ['SRR13586007', 'SRR13586026']\n",
    "\n",
    "SEs = ['SRR13586106']\n",
    "PEs = []\n",
    "datasets = []\n",
    "\n",
    "for SE in SEs:\n",
    "    vcf_link = f\"/home/ndo/nextflow_SE/outDir/VarCall/{SE}_vardict.vcf\"\n",
    "    datasets.append(vcf_link)\n",
    "\n",
    "for PE in PEs:\n",
    "    vcf_link = f\"/home/ndo/nextflow/outDir/VarCall/{PE}_vardict.vcf\"\n",
    "    datasets.append(vcf_link)\n",
    "\n",
    "\n",
    "giab_to_dataset = {'NA12878': {'SRR13586007','SRR13586016','SRR13586106'}, 'NA24695': {'SRR13586011'}, 'NA24694': {'SRR13586012','SRR13586013','SRR13586014', 'SRR13586015'}, \\\n",
    "           'NA24631': {'SRR13586018', 'SRR13586019', 'SRR13586020'}, 'NA24149': {'SRR13586026'}}\n",
    "\n",
    "# Create an empty dictionary to store the reversed mapping\n",
    "dataset_to_giab = {}\n",
    "\n",
    "# Iterate through the original dictionary\n",
    "for giab, ids in giab_to_dataset.items():\n",
    "    # For each dataset, map it to the corresponding GIAB value\n",
    "    for id in ids:\n",
    "        dataset_to_giab[id] = giab\n",
    "dataset_to_giab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ndo/miniconda3/envs/notebook/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "\n",
    "def vcf_to_df(vcf_file, extract_flank_seqs=True):\n",
    "    \"\"\" Converts vcf file into a pandas dataframe\n",
    "    \"\"\"\n",
    "    with open(vcf_file) as vcf:\n",
    "        for line in vcf:\n",
    "            if line.startswith('#CHROM'):\n",
    "                header_names = line.strip().split()\n",
    "                header_names[0] = header_names[0][1:]\n",
    "                break\n",
    "        read_lines = vcf.readlines()\n",
    "        # list containing each row as 1 giant string\n",
    "\n",
    "    \n",
    "    # split the giant string into columns -- now we have a 2D list\n",
    "    create_columns = [row.strip().split() for row in read_lines]\n",
    "\n",
    "    final = []\n",
    "    \n",
    "    # extracts LSEQ and RSEQ for SRR vcf file if needed, in addition to CHROM, POS, REF, ALT\n",
    "    if extract_flank_seqs == True:\n",
    "        for row in create_columns:\n",
    "            info = row[7].split(';')\n",
    "            if info[1] != 'TYPE=SNV':\n",
    "                continue\n",
    "            final.append([row[0], row[1], row[3], row[4], row[7]]) \n",
    "        df = pd.DataFrame(data=final, columns=header_names[0:2]+header_names[3:5]+['INFO'])\n",
    "        return df\n",
    "    \n",
    "    # extracts CHROM, POS, REF, ALT    \n",
    "    for row in create_columns:\n",
    "        final.append([row[0], row[1], row[3], row[4]]) # CHROM, POS, REF, ALT                     \n",
    "    df = pd.DataFrame(data=final, columns=header_names[0:2]+header_names[3:5])   \n",
    "    return df\n",
    "\n",
    "def parse_and_convert_type(df):\n",
    "\n",
    "\n",
    "    # Assuming your DataFrame is named df\n",
    "    # Split the INFO column by semicolons to get key-value pairs\n",
    "    info_split = df['INFO'].str.split(';')\n",
    "\n",
    "    # Create a dictionary to store the key-value pairs\n",
    "    info_dict = {}\n",
    "\n",
    "    # Iterate through each row of the split INFO column\n",
    "    for row in info_split:\n",
    "\n",
    "        # Filter out SNV:\n",
    "        if row[1] != 'TYPE=SNV':\n",
    "            continue\n",
    "        # Iterate through each key-value pair in the row\n",
    "        for item in row:\n",
    "            # Split the key-value pair by '='\n",
    "            key, value = item.split('=')\n",
    "            # Add the key-value pair to the dictionary\n",
    "            if key in info_dict:\n",
    "                info_dict[key].append(value)\n",
    "            else:\n",
    "                info_dict[key] = [value]\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    info_df = pd.DataFrame(info_dict)\n",
    "\n",
    "    # Rename the columns\n",
    "    info_df.rename(columns={'TYPE': 'TYPE', 'DP': 'DP', 'VD': 'VD', 'AF': 'AF', 'BIAS': 'BIAS', 'REFBIAS': 'REFBIAS', 'VARBIAS': 'VARBIAS', 'PMEAN': 'PMEAN', 'PSTD': 'PSTD', 'QUAL': 'QUAL', 'QSTD': 'QSTD', 'SBF': 'SBF', 'ODDRATIO': 'ODDRATIO', 'MQ': 'MQ', 'SN': 'SN', 'HIAF': 'HIAF', 'ADJAF': 'ADJAF', 'SHIFT3': 'SHIFT3', 'MSI': 'MSI', 'MSILEN': 'MSILEN', 'NM': 'NM', 'HICNT': 'HICNT', 'HICOV': 'HICOV', 'LSEQ': 'LSEQ', 'RSEQ': 'RSEQ', 'DUPRATE': 'DUPRATE', 'SPLITREAD': 'SPLITREAD', 'SPANPAIR': 'SPANPAIR'}, inplace=True)\n",
    "\n",
    "    # Merge with the original DataFrame\n",
    "    df = pd.concat([df, info_df], axis=1)\n",
    "\n",
    "    # Drop the original INFO column\n",
    "    df.drop(columns=['INFO'], inplace=True)\n",
    "\n",
    "\n",
    "    # Convert CHROM and POS columns to create MultiIndex\n",
    "    df.index = pd.MultiIndex.from_tuples(zip(df['CHROM'], df['POS']))\n",
    "    # df.drop(['CHROM', 'POS'], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    # Convert columns to desired data types\n",
    "    df['POS'] = df['POS'].astype('int64')\n",
    "    df['AF'] = df['AF'].astype(float)\n",
    "    df['ADJAF'] = df['ADJAF'].astype(float)\n",
    "    df['DP'] = df['DP'].astype(int)\n",
    "    df['DUPRATE'] = df['DUPRATE'].astype(int)\n",
    "    df['HIAF'] = df['HIAF'].astype(float)\n",
    "    df['HICNT'] = df['HICNT'].astype(int)\n",
    "    df['HICOV'] = df['HICOV'].astype(int)\n",
    "    df['MQ'] = df['MQ'].astype(float)\n",
    "    df['MSI'] = df['MSI'].astype(float)\n",
    "    df['MSILEN'] = df['MSILEN'].astype(int)\n",
    "    df['NM'] = df['NM'].astype(float)\n",
    "    df['ODDRATIO'] = df['ODDRATIO'].astype(float)\n",
    "    df['PMEAN'] = df['PMEAN'].astype(float)\n",
    "    df['PSTD'] = df['PSTD'].astype(int)\n",
    "    df['QSTD'] = df['QSTD'].astype(int)\n",
    "    df['QUAL'] = df['QUAL'].astype(float)\n",
    "    df['SBF'] = df['SBF'].astype(float)\n",
    "    df['SHIFT3'] = df['SHIFT3'].astype(int)\n",
    "    df['SN'] = df['SN'].astype(float)\n",
    "    df['SPANPAIR'] = df['SPANPAIR'].astype(int)\n",
    "    df['SPLITREAD'] = df['SPLITREAD'].astype(int)\n",
    "    return df\n",
    "\n",
    "def preprocessing_sample_dataset(vcf_file):\n",
    "    df = vcf_to_df(vcf_file, extract_flank_seqs=True)\n",
    "    return parse_and_convert_type(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset /home/ndo/nextflow_SE/outDir/VarCall/SRR13586106_vardict.vcf\n"
     ]
    }
   ],
   "source": [
    "sampleId_to_df = {}\n",
    "for dataset in datasets:\n",
    "    # Split the path and extract the sample ID (assuming format like SRR<sample_id>_vardict.vcf)\n",
    "    print(\"dataset\", dataset)\n",
    "    sample_id = dataset.split('/')[-1].split('_')[0]\n",
    "    df = preprocessing_sample_dataset(dataset)\n",
    "    sampleId_to_df[sample_id] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess GIAB\n",
    "\n",
    "#Convert GIAB file to process SNP only\n",
    "\n",
    "# %%bcftools view -v snps /home/ndo/GIAB-GT/GIAB_NA24149.vcf > GIAB_SNP_NA24149.vcf\n",
    "\n",
    "# %%bcftools view -v snps /home/ndo/GIAB-GT/GIAB_NA24631.vcf > GIAB_SNP_NA24631.vcf\n",
    "# %%bcftools view -v snps /home/ndo/GIAB-GT/GIAB_NA24694.vcf > GIAB_SNP_NA24694.vcf\n",
    "# %%bcftools view -v snps /home/ndo/GIAB-GT/GIAB_NA24695.vcf > GIAB_SNP_NA24695.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vcf_with_header(vcf_file):\n",
    "    \"\"\"\n",
    "    Read VCF file into pandas DataFrame and extract header lines.\n",
    "    \"\"\"\n",
    "    vcf_header = []\n",
    "    with open(vcf_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#CHROM'):\n",
    "                vcf_header.extend(line.strip().split('\\t'))\n",
    "    df = pd.read_csv(vcf_file, sep=\"\\t\", comment=\"#\")\n",
    "    df.columns = vcf_header\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "giab_datasets = ['/home/ndo/GIAB-GT/GIAB_SNP_NA12878.vcf', '/home/ndo/GIAB-GT/GIAB_SNP_NA24149.vcf', '/home/ndo/GIAB-GT/GIAB_SNP_NA24631.vcf', '/home/ndo/GIAB-GT/GIAB_SNP_NA24694.vcf', '/home/ndo/GIAB-GT/GIAB_SNP_NA24695.vcf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hashmap to store sample ID as key and corresponding DataFrame as value\n",
    "hashmap = {}\n",
    "\n",
    "for dataset in giab_datasets:\n",
    "    # Extract the sample ID from the file path (assuming format like GIAB_SNP_<sample_id>.vcf)\n",
    "    sample_id = dataset.split('_')[-1].split('.')[0]\n",
    "    \n",
    "    # Read the VCF file into a DataFrame\n",
    "    df = read_vcf_with_header(dataset)\n",
    "    df= df[['#CHROM','POS','REF','ALT']]\n",
    "    df.rename(columns={'#CHROM': 'CHROM'}, inplace=True)\n",
    "    # Store the DataFrame in the hashmap with the sample ID as the key\n",
    "    hashmap[sample_id] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SRR13586106'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleId_to_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NA12878':          CHROM       POS REF ALT\n",
       " 0         chr1    783175   T   C\n",
       " 1         chr1    784860   T   C\n",
       " 2         chr1    785417   G   A\n",
       " 3         chr1    797392   G   A\n",
       " 4         chr1    798618   C   T\n",
       " ...        ...       ...  ..  ..\n",
       " 3358496  chr22  50791190   G   A\n",
       " 3358497  chr22  50792075   G   T\n",
       " 3358498  chr22  50792591   T   C\n",
       " 3358499  chr22  50792792   A   G\n",
       " 3358500  chr22  50793229   T   C\n",
       " \n",
       " [3358501 rows x 4 columns],\n",
       " 'NA24149':          CHROM       POS REF ALT\n",
       " 0         chr1    602493   C   T\n",
       " 1         chr1    602494   A   G\n",
       " 2         chr1    779047   G   A\n",
       " 3         chr1    779968   T   G\n",
       " 4         chr1    783006   A   G\n",
       " ...        ...       ...  ..  ..\n",
       " 3432730  chr22  50791190   G   A\n",
       " 3432731  chr22  50791289   A   T\n",
       " 3432732  chr22  50792075   G   T\n",
       " 3432733  chr22  50792792   A   G\n",
       " 3432734  chr22  50793229   T   C\n",
       " \n",
       " [3432735 rows x 4 columns],\n",
       " 'NA24631':          CHROM       POS REF ALT\n",
       " 0         chr1    623924   A   G\n",
       " 1         chr1    627528   A   G\n",
       " 2         chr1    628245   T   C\n",
       " 3         chr1    629660   C   T\n",
       " 4         chr1    637944   C   A\n",
       " ...        ...       ...  ..  ..\n",
       " 3388382  chr22  50778305   C   T\n",
       " 3388383  chr22  50782013   T   C\n",
       " 3388384  chr22  50788602   A   G\n",
       " 3388385  chr22  50789509   A   C\n",
       " 3388386  chr22  50793229   T   C\n",
       " \n",
       " [3388387 rows x 4 columns],\n",
       " 'NA24694':          CHROM       POS REF ALT\n",
       " 0         chr1    627528   A   G\n",
       " 1         chr1    628245   T   C\n",
       " 2         chr1    641034   C   T\n",
       " 3         chr1    778622   C   G\n",
       " 4         chr1    779047   G   A\n",
       " ...        ...       ...  ..  ..\n",
       " 3370137  chr22  50782013   T   C\n",
       " 3370138  chr22  50784803   G   C\n",
       " 3370139  chr22  50788602   A   G\n",
       " 3370140  chr22  50789509   A   C\n",
       " 3370141  chr22  50793229   T   C\n",
       " \n",
       " [3370142 rows x 4 columns],\n",
       " 'NA24695':          CHROM       POS REF ALT\n",
       " 0         chr1    604137   T   G\n",
       " 1         chr1    604358   G   C\n",
       " 2         chr1    604360   T   C\n",
       " 3         chr1    606038   G   A\n",
       " 4         chr1    606389   A   G\n",
       " ...        ...       ...  ..  ..\n",
       " 3387387  chr22  50784803   G   C\n",
       " 3387388  chr22  50788602   A   G\n",
       " 3387389  chr22  50789509   A   C\n",
       " 3387390  chr22  50791427   G   A\n",
       " 3387391  chr22  50793229   T   C\n",
       " \n",
       " [3387392 rows x 4 columns]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SRR13586016': 'NA12878',\n",
       " 'SRR13586007': 'NA12878',\n",
       " 'SRR13586011': 'NA24695',\n",
       " 'SRR13586014': 'NA24694',\n",
       " 'SRR13586012': 'NA24694',\n",
       " 'SRR13586015': 'NA24694',\n",
       " 'SRR13586013': 'NA24694',\n",
       " 'SRR13586020': 'NA24631',\n",
       " 'SRR13586019': 'NA24631',\n",
       " 'SRR13586018': 'NA24631',\n",
       " 'SRR13586026': 'NA24149'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_giab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dfs = {}\n",
    "for sample_id_local, giab_id in dataset_to_giab.items():\n",
    "    if sample_id_local not in sampleId_to_df:\n",
    "        continue\n",
    "    sample_df = sampleId_to_df[sample_id_local]\n",
    "    giab_df = hashmap[giab_id]\n",
    "    merge_df = pd.merge(sample_df, giab_df, how=\"outer\", on=[\"CHROM\", \"POS\"])\n",
    "    merge_dfs[sample_id_local] = merge_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_variants(df):\n",
    "    if (df[\"ALT_GIAB\"] == df[\"ALT_S\"]) and (df[\"REF_GIAB\"] == df[\"REF_S\"]):\n",
    "        return \"TP\"\n",
    "    if (df[\"ALT_GIAB\"] != df[\"ALT_S\"]) and (df[\"REF_GIAB\"] == df[\"REF_S\"]):\n",
    "        return \"FP\"\n",
    "    if  df[\"ALT_GIAB\"] and pd.isna(df[\"ALT_S\"]):\n",
    "        return \"FN\"\n",
    "    if  pd.isna(df[\"ALT_GIAB\"]) and df[\"ALT_S\"]:\n",
    "        return \"FP\"\n",
    "    return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SRR13586106':         CHROM        POS REF_x ALT_x SAMPLE TYPE  DP   VD  AF BIAS  ...  NM  \\\n",
       " 0        chr1     783175   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " 1        chr1     784860   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " 2        chr1     785417   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " 3        chr1     797392   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " 4        chr1     798618   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " ...       ...        ...   ...   ...    ...  ...  ..  ...  ..  ...  ...  ..   \n",
       " 3365669  chr9  138216697   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " 3365670  chr9  138217446   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " 3365671  chr9  138217868   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " 3365672  chr9  138219722   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " 3365673  chr9  138221925   NaN   NaN    NaN  NaN NaN  NaN NaN  NaN  ... NaN   \n",
       " \n",
       "         HICNT  HICOV  LSEQ  RSEQ  DUPRATE  SPLITREAD  SPANPAIR  REF_y  ALT_y  \n",
       " 0         NaN    NaN   NaN   NaN      NaN        NaN       NaN      T      C  \n",
       " 1         NaN    NaN   NaN   NaN      NaN        NaN       NaN      T      C  \n",
       " 2         NaN    NaN   NaN   NaN      NaN        NaN       NaN      G      A  \n",
       " 3         NaN    NaN   NaN   NaN      NaN        NaN       NaN      G      A  \n",
       " 4         NaN    NaN   NaN   NaN      NaN        NaN       NaN      C      T  \n",
       " ...       ...    ...   ...   ...      ...        ...       ...    ...    ...  \n",
       " 3365669   NaN    NaN   NaN   NaN      NaN        NaN       NaN      C      G  \n",
       " 3365670   NaN    NaN   NaN   NaN      NaN        NaN       NaN      C      T  \n",
       " 3365671   NaN    NaN   NaN   NaN      NaN        NaN       NaN      T      C  \n",
       " 3365672   NaN    NaN   NaN   NaN      NaN        NaN       NaN      A      G  \n",
       " 3365673   NaN    NaN   NaN   NaN      NaN        NaN       NaN      G      A  \n",
       " \n",
       " [3365674 rows x 35 columns]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, m_df in merge_dfs.items():\n",
    "    m_df = m_df.rename(columns={\"REF_x\" : \"REF_GIAB\", \"ALT_x\" : \"ALT_GIAB\", \"REF_y\" : \"REF_S\", \"ALT_y\" : \"ALT_S\"})\n",
    "    # m_df['VAR_CATE'] = m_df.swifter.apply(categorize_variants, axis = 1)\n",
    "    merge_dfs[id] = m_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 3365674/3365674 [01:10<00:00, 48001.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for id, m_df in merge_dfs.items():\n",
    "    m_df['VAR_CATE'] = m_df.swifter.apply(categorize_variants, axis = 1)\n",
    "    merge_dfs[id] = m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP the INDELS in GIAB\n",
    "for id, m_df in merge_dfs.items():\n",
    "    #DROP the INDELS in GIAB\n",
    "    m_df.drop(m_df[m_df.VAR_CATE == 'None'].index, inplace=True)\n",
    "    merge_dfs[id] = m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for SRR13586106 written to /home/ndo/vardict_ML/mergeDf_pre_filter_FN/SRR13586106_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessing dataframe to a text file\n",
    "# Iterate through the merge_dfs dictionary\n",
    "for sample_id, df in merge_dfs.items():\n",
    "    # Construct the CSV file name\n",
    "    csv_file_name = f\"/home/ndo/vardict_ML/mergeDf_pre_filter_FN/{sample_id}_df.csv\"\n",
    "    # Write the DataFrame to the CSV file\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    print(f\"Data for {sample_id} written to {csv_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
